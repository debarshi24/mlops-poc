"""
deploy_model.py

Deploy a model artifact (joblib/pkl or model.tar.gz) from S3 to a SageMaker real-time endpoint.

Expected environment variables (provided by CloudFormation/CodeBuild):
- S3_BUCKET              REQUIRED (artifact bucket)
- SAGEMAKER_ROLE_ARN     REQUIRED (SageMaker execution role ARN)
- AWS_DEFAULT_REGION     REQUIRED
Optional:
- MODEL_S3_URI           explicit s3://... path to model.joblib or model.tar.gz
- INSTANCE_TYPE          e.g. ml.t3.medium (default ml.t3.medium)
- ENDPOINT_NAME          desired endpoint name (default autogenerated)
- CLEANUP_AFTER_DEPLOY   'true' to delete endpoint after creation (for tests) (default 'false')
"""
import os
import logging
import boto3
import tarfile
import tempfile
import uuid
from urllib.parse import urlparse
from datetime import datetime

import sagemaker
from sagemaker.sklearn.model import SKLearnModel

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("deploy_model")

def parse_s3_uri(s3_uri):
    parsed = urlparse(s3_uri)
    if parsed.scheme != "s3":
        raise ValueError("MODEL_S3_URI must be an s3:// URI")
    bucket = parsed.netloc
    key = parsed.path.lstrip("/")
    return bucket, key

def download_s3_file(s3_client, bucket, key, local_path):
    os.makedirs(os.path.dirname(local_path), exist_ok=True)
    s3_client.download_file(bucket, key, local_path)
    return local_path

def upload_file_to_s3(s3_client, local_path, bucket, key):
    s3_client.upload_file(local_path, bucket, key)
    return f"s3://{bucket}/{key}"

def find_latest_model_s3(s3_client, bucket, prefix="models/"):
    paginator = s3_client.get_paginator("list_objects_v2")
    keys = []
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for obj in page.get("Contents", []):
            k = obj["Key"]
            if k.endswith("model.joblib") or k.endswith(".joblib") or k.endswith(".pkl") or k.endswith("model.pkl"):
                keys.append(k)
            if k.endswith("model.tar.gz"):
                keys.append(k)
    if not keys:
        return None
    keys.sort()
    return f"s3://{bucket}/{keys[-1]}"

def ensure_model_tar(s3_client, model_s3_uri, bucket, s3_prefix_for_deployment):
    """
    Ensure the model artifact is a model.tar.gz in S3.
    If model_s3_uri already points to a .tar.gz, return it.
    If it points to a joblib/pkl, download, package into model.tar.gz with model.joblib inside, and upload.
    Returns s3://bucket/<key-to-tar.gz>
    """
    if model_s3_uri and model_s3_uri.endswith(".tar.gz"):
        return model_s3_uri

    # Parse model_s3_uri or assume latest in bucket
    if model_s3_uri:
        model_bucket, model_key = parse_s3_uri(model_s3_uri)
    else:
        # find latest model in bucket
        found = find_latest_model_s3(s3_client, bucket)
        if not found:
            raise RuntimeError("No model artifact found in bucket under models/ prefix")
        model_bucket, model_key = parse_s3_uri(found)

    # If it's already a tar.gz
    if model_key.endswith(".tar.gz"):
        return f"s3://{model_bucket}/{model_key}"

    # Download the raw model file
    tmpdir = tempfile.mkdtemp()
    local_model_path = os.path.join(tmpdir, os.path.basename(model_key))
    logger.info("Downloading model from s3://%s/%s to %s", model_bucket, model_key, local_model_path)
    download_s3_file(s3_client, model_bucket, model_key, local_model_path)

    # Create tar.gz with model.joblib (SageMaker convention)
    tar_name = f"model-{uuid.uuid4().hex}.tar.gz"
    local_tar_path = os.path.join(tmpdir, tar_name)
    logger.info("Creating tar.gz %s with model file inside named 'model.joblib'", local_tar_path)
    with tarfile.open(local_tar_path, "w:gz") as tar:
        arcname = "model.joblib"
        tar.add(local_model_path, arcname=arcname)

    # Upload tar.gz to deployment prefix
    deploy_key = f"{s3_prefix_for_deployment.rstrip('/')}/{tar_name}"
    s3_upload_path = upload_file_to_s3(s3_client, local_tar_path, bucket, deploy_key)
    logger.info("Uploaded packaged model to %s", s3_upload_path)
    return s3_upload_path

def create_and_deploy(endpoint_name, model_data_s3_uri, role_arn, region, instance_type="ml.t3.medium"):
    sess = sagemaker.Session(boto_session=boto3.Session(region_name=region))
    entry_point = "src/model/inference.py"  # ensure this exists in repo
    model = SKLearnModel(
        model_data=model_data_s3_uri,
        role=role_arn,
        entry_point=entry_point,
        framework_version="0.23-1",
        sagemaker_session=sess,
        name=f"mlops-sklearn-model-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"
    )

    logger.info("Creating endpoint config and deploying to endpoint '%s' (instance_type=%s)", endpoint_name, instance_type)
    predictor = model.deploy(
        initial_instance_count=1,
        instance_type=instance_type,
        endpoint_name=endpoint_name,
        wait=True
    )
    logger.info("Endpoint created: %s", endpoint_name)
    return endpoint_name, predictor

def delete_endpoint(endpoint_name, region):
    sm = boto3.client("sagemaker", region_name=region)
    try:
        logger.info("Deleting endpoint %s", endpoint_name)
        sm.delete_endpoint(EndpointName=endpoint_name)
    except Exception as e:
        logger.warning("Error deleting endpoint: %s", e)
    # also delete config if desired
    try:
        cfg_name = endpoint_name + "-config"
        sm.delete_endpoint_config(EndpointConfigName=cfg_name)
    except Exception:
        pass

def main():
    bucket = os.environ.get("S3_BUCKET")
    role_arn = os.environ.get("SAGEMAKER_ROLE_ARN")
    region = os.environ.get("AWS_DEFAULT_REGION")
    explicit_model_s3 = os.environ.get("MODEL_S3_URI")  # optional
    instance_type = os.environ.get("INSTANCE_TYPE", "ml.t3.medium")
    endpoint_name = os.environ.get("ENDPOINT_NAME") or f"mlops-endpoint-{datetime.utcnow().strftime('%Y%m%d%H%M%S')}"
    cleanup = os.environ.get("CLEANUP_AFTER_DEPLOY", "false").lower() == "true"

    if not bucket or not role_arn or not region:
        raise ValueError("S3_BUCKET, SAGEMAKER_ROLE_ARN and AWS_DEFAULT_REGION must be set")

    s3_client = boto3.client("s3", region_name=region)

    # Prepare a deployable model.tar.gz in S3
    model_data_s3_uri = ensure_model_tar(s3_client, explicit_model_s3, bucket, s3_prefix_for_deployment="models/deployed")
    logger.info("Model data prepared at: %s", model_data_s3_uri)

    # Create & deploy model (using SageMaker SKLearnModel)
    try:
        endpoint, predictor = create_and_deploy(endpoint_name, model_data_s3_uri, role_arn, region, instance_type)
        logger.info("Deployed endpoint: %s", endpoint)
    except Exception as e:
        logger.exception("Failed to create or deploy endpoint: %s", e)
        raise

    if cleanup:
        delete_endpoint(endpoint, region)
        logger.info("Cleanup requested â€” endpoint deleted")

if __name__ == "__main__":
    main()